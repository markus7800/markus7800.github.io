<!DOCTYPE html>

<html lang="en">
	<head>
		<meta charset="UTF-8">
		<link rel="stylesheet" type="text/css" href="./blog.css">
		<link rel="icon" type="image/png" href="./res/m.png">
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

		\(
			\newcommand{\bfx}{\mathbf{x}}
			\newcommand{\bfX}{\mathbf{X}}
			\newcommand{\bfy}{\mathbf{y}}
			\newcommand{\bfz}{\mathbf{z}}
			\newcommand{\bfw}{\mathbf{w}}
			\newcommand{\bft}{\mathbf{t}}
			\newcommand{\N}{\mathcal{N}}
			\newcommand{\D}{\mathcal{D}}
			\newcommand{\bfPhi}{\mathbf{\Phi}}
			\newcommand{\bfI}{\mathbf{I}}
			\newcommand{\bfm}{\mathbf{m}}
			\newcommand{\bfS}{\mathbf{S}}
			\newcommand{\bfL}{\mathbf{L}}
			\newcommand{\bfA}{\mathbf{A}}
			\newcommand{\bfb}{\mathbf{b}}
			\newcommand{\d}{\text{d}}
			\newcommand{\Exp}[2][]{\mathbb{E}_{#1}\left[#2\right]}
		\)

		<title>Linear Regression</title>
	</head>
	<body>
		<div>
		<p>
			<a href="./blog.html">Back</a>
		</p>
		<h1>Linear Regression</h1>
		<main>
			<h2>Model</h2>
			<p>
				Define the linear regression function as
				\[y(\bfx;\bfw) = \bfw^T \phi(\bfx)\]
				with basis functions \(\phi = (\phi_1, \dots, \phi_d)^T\) and \(\phi_0(\bfx) := 1\).
			</p>
			<p>
				We assume that our variables follow a linear model with Gaussian noise \(\epsilon \sim \N(0, \beta^{-1})\) such that
				\[y = y(\bfx; \bfw) + \epsilon\] for some unknown \(\bfw.\)
			</p>

			<h2>Maximum Likelihood Estimation</h2>
			<p>
				If we have observed data \(\bfX = (\bfx_1, \dots \bfx_N)^T\), \(\bfy = (y_1, \dots, y_N)^T\) independently, we can find \(\bfw\) such that we maximise the probability of having observed our data.
			</p>
			<p>
				The probability of observing given our data and parameters (<i>likelihood</i>) is
				\[p(\bfy|\bfX,\bfw,\beta) = \prod_{n=1}^N \N(y_n|y(\bfx_n;\bfw), \beta^{-1}).\]
				Maximising this quantity is equivalent to maximising its logarithm (<i>log-likelihood</i>)
				\[\ln P(\bfy|\bfX,\bfw,\beta) = \frac{N}{2}\ln\beta - \frac{N}{2}\ln(2\pi)-\beta E_D(\bfw),\]
				\[E_D(\bfw) = \frac{1}{2}\sum_{n=1}^N(y_n-\bfw^T\phi(\bfx_n))^2.\]
				We see that the maximum likelihood estimation of \(\bfw\) can be found by minimising the sum of squared residuals.
			</p>
			<p>
				We can solve for \(\bfw\) analytically and obtain
				\[\bfw_{\text{ML}} = (\bfPhi ^T\bfPhi)^{-1}\bfPhi^T \bfy\]
				where
				\begin{align}
					\bfPhi = (\phi(\bfx_1),\dots, \phi(\bfx_N))^T =
					\begin{pmatrix}
					1 & \phi_1(\bfx_1) & \dots & \phi_d(\bfx_1) \\
					1 & \phi_1(\bfx_2) & \dots & \phi_d(\bfx_2) \\
					\vdots &  & & \vdots \\
					1 & \phi_1(\bfx_N) & \dots & \phi_d(\bfx_N)
					\end{pmatrix}
				\end{align}
			</p>
			<p>
				To avoid overfitting we can minimise the sum of squared residuals plus a regularisation term
				\[E_D(\bfw) + \lambda E_W(\bfw), \quad E_W(\bfw) = \frac{1}{2}\bfw^T\bfw.\]
				Then we have
				\[\bfw_{\text{ML}}^\lambda = (\lambda \bfI + \bfPhi^T \bfPhi)^{-1}\bfPhi^T \bfy.\]
			</p>
			<button class="collapsible">Proof.</button>
			<div class="collapsiblecontent">
				<p>
					\begin{align}
					\nabla_\bfw L(\bfw) & =
						\nabla_\bfw E_D(\bfw) + \lambda E_W(\bfw) \\

						&= \nabla_\bfw \frac{1}{2}((\bfy - \bfPhi \bfw)^T(\bfy - \bfPhi \bfw) +  \lambda \bfw^T\bfw) \\

						&= -\bfPhi^T (\bfy - \bfPhi \bfw) + \lambda \bfw \\

						& = -\bfPhi^T\bfy + (\lambda\bfI + \bfPhi^T\bfPhi) \bfw
					\end{align}
					\begin{align}
					\nabla^2_\bfw L(\bfw) = \lambda\bfI + \bfPhi^T\bfPhi
					\end{align}

					Thus \(\bfw = (\lambda \bfI + \bfPhi^T \bfPhi)^{-1}\bfPhi^T \bfy\) is a minimum as the Hessian is positive definite if \(\bfPhi\) is regular.
				</p>
				<p align="right">\(\Box\)</p>
			</div>

			<h2>Bayesian Estimation</h2>
			<p>
				When using the Bayesian framework we need a prior over the parameters \(p(\bfw)\) and use the observed data \(\D\) to update our knowledge of the paramters with the help of Bayes rule
				\[p(\bfw | \D) = \frac{p(\D | \bfw)p(\bfw)}{p(\D)}\]
			</p>
			<p>
				We know that
				\[p(\D | \bfw) = \prod_{n=1}^N \N(y_n|y(\bfx_n;\bfw), \beta^{-1})\]
				is the exponential of a quadratic function \(\bfw\) and therefore the conjugate prior is a given by a Gaussian distribution
				\[p(\bfw) = \N(\bfw | \bfm_0, \bfS_0)\]
				with
				\[p(\bfw|\D) = \N(\bfw|\bfm_N, \bfS_N)\]
				where
				\begin{align}
				\bfm_N &= \bfS_N(\bfS_0^{-1}\bfm_0 + \beta \bfPhi^T \bfy) \\
				\bfS_N^{-1} &= \bfS_0^{-1} + \beta \bfPhi^T \bfPhi.
				\end{align}
			</p>

			<button class="collapsible">Proof.</button>
			<div class="collapsiblecontent">
				<p>
					\begin{align}
					-2\ln p(\D | \bfw) p(\bfw) &= 
					\beta (\bfy - \bfPhi\bfw)^T(\bfy - \bfPhi\bfw)
					 \\
					&\quad\, +(\bfw- \bfm_0)^T\bfS_0^{-1}(\bfw - \bfm_0) + \text{const} \\

					&= \beta\bfy^T\bfy - \beta2\bfy^T\bfPhi\bfw + \beta\bfw^T\bfPhi^T\bfPhi \bfw \\
					&\quad\, + \bfw\bfS_0^{-1}\bfw - 2\bfw^T\bfS_0^{-1}\bfm_0 + \bfm_0^T\bfS_0^{-1}\bfm_0 + \text{c.} \\

					&= \bfw^T \bfS_N^{-1}\bfw - 2\bfw^T S_N^{-1} S_N (\beta \bfPhi^T\bfy + \bfS_0^{-1}\bfm_0) + \text{c.} \\

					&= \bfw^T \bfS_N^{-1}\bfw - 2\bfw^T S_N^{-1} \bfm_N + \bfm_N^T S_N^{-1} \bfm_N + \text{c.} \\

					& = (\bfw^T - \bfm_N)^T \bfS_N^{-1}(\bfw^T - \bfm_N)
					\end{align}
				</p>
				<p align="right">\(\Box\)</p>
			</div>

			<p>
				If we restrict the prior to a zero-mean isotropic Gaussian, \(P(\bfw|\alpha) = \N(\bfw| \mathbf{0}, \alpha^{-1}\bfI)\), we have

				\begin{align}
				\bfm_N &= \beta\bfS_N \bfPhi^T \bfy\\
				\bfS_N^{-1} &= \alpha \bfI + \beta \bfPhi^T \bfPhi.
				\end{align}

				In this case we have
				\[\ln P(\bfw|\D) = -\frac{\beta}{2}\sum_{n=1}^N(y_n-\bfw^T\phi(\bfx_n))^2 - \frac{\alpha}{2}\bfw^T\bfw + \text{const}. \]
				Therefore, maximising the posterior is equivalent to the maximum likelihood estimation with regularisation \(\lambda = \frac{\alpha}{\beta}\)
				\[\bfw_{\text{MAP}}^\alpha = \bfw_\text{ML}^\lambda.\]

				Letting \(\alpha \to 0\) (making the prior broader) is equivalent to the maximum likelihood estimation without regularisation,
				\[\lim_{\alpha \to 0}\bfw_{\text{MAP}}^\alpha = \bfw_\text{ML}.\]
			</p>

			<h3>Predictive distribution</h3>
			<p>
				In the Bayesian framework we are usually not interested in a point estimate of \(\bfw\) but want to directly make predictions by averageing over all parameters. The <i>predictive distribution at \(\bfx\)</i> is given by
				\[P(y|\bfx, \D, \alpha, \beta) = \int P(y|\bfx, \bfw, \beta)P(\bfw|\D,\alpha,\beta) \text{d}\bfw.\]
				Recall that
				\[P(y|\bfx, \bfw, \beta) = \N(y|y(\bfx;\bfw), \beta^{-1}), \quad P(\bfw|\D,\alpha,\beta) = \N(\bfw|\bfm_N, \bfS_N).\]
				Thus,
				\[P(y|\bfx, \D, \alpha, \beta) = \N(y|y(\bfx; \bfm_N), \sigma^2_N(\bfx))\]
				where
				\begin{align}
				y(\bfx; \bfm_N) = \bfm_N^T \phi(\bfx), \quad \sigma_N^2(\bfx) = \frac{1}{\beta} + \phi(\bfx)^T \bfS_N \phi(\bfx).
				\end{align}
			</p>

			<button class="collapsible">Proof.</button>
			<div class="collapsiblecontent">
				<h3>Claim.</h3>
				<p>
					For \(p(\bfx)=\N(\bfx | \bfm, \bfS)\), \(p(\bfy|\bfx) = \N(\bfA \bfx + \bfb, \bfL)\) then

					\begin{align}
					p(\bfy) = \N(\bfy | \bfA\bfm + \bfb, \bfL + \bfA \bfS \bfA^T).
					\end{align}
				</p>

				<p style="font-style: italic">Proof.</p>
				<p>
					Set \(\bfz = (\bfx, \bfy)^T\). We use following decomposition
					\begin{align}
					-\frac{1}{2}(\bfz - \mu)^T\Sigma^{-1}(\bfz - \mu) = -\frac{1}{2} \bfz^T\Sigma^{-1} \bfz + \bfz^T \Sigma^{-1} \mu + \text{const.}
					\end{align}
				</p>
				<p>
					We look at the quadratic terms.
					\begin{align}
					\ln p(\bfz) &= \ln p(\bfx) + \ln p(\bfy|\bfx) \\

					&= -\frac{1}{2}(\bfx - \bfm)^T \bfS^{-1}(\bfx - \bfm)\\
					&\quad\,-\frac{1}{2}(\bfy - \bfA\bfx - \bfb)^T \bfL^{-1}(\bfy - \bfA\bfx - \bfb) + \text{const} \\

					&=  -\frac{1}{2}\bfx^T(\bfS^{-1} + \bfA^T \bfL^{-1} \bfA)\bfx -\frac{1}{2} \bfy^T \bfL^{-1}\bfy\\
					&\quad\, + \frac{1}{2}\bfy^T\bfL^{-1}\bfA \bfx + \frac{1}{2} \bfx^T\bfA^T\bfL^{-1}\bfy + \dots \\

					&= -\frac{1}{2} \begin{pmatrix} \bfx \\ \bfy \end{pmatrix}^T
					\begin{pmatrix}
					\bfS^{-1} + \bfA^T \bfL^{-1} \bfA & -\bfA^T\bfL^{-1} \\
					-\bfL^{-1}\bfA & \bfL^{-1}
					\end{pmatrix}
					\begin{pmatrix} \bfx \\ \bfy \end{pmatrix}
					+ \dots
					\end{align}

					Inverting this matrix gives
					\begin{align}
					\operatorname{cov}[\bfz] = 
					\begin{pmatrix}
					\bfS & \bfS \bfA^T \\
					\bfA \bfS & \bfL + \bfA \bfS \bfA^T
					\end{pmatrix}.
					\end{align}

					The linear terms are
					\begin{align}
					\bfx^T \bfS^{-1} \bfm - \bfx^T \bfA^T \bfL^{-1}\bfb + \bfy^T\bfL^{-1} \bfb = 
					\begin{pmatrix} \bfx \\ \bfy \end{pmatrix}^T
					\begin{pmatrix} \bfS^{-1}\bfm - \bfA^T \bfL^{-1} \bfb\\ \bfL^{-1}\bfb \end{pmatrix}.
					\end{align}

					Therefore
					\begin{align}
					\Exp{\bfz} = \begin{pmatrix} \bfm \\ \bfA \bfm + \bfb \end{pmatrix}.
					\end{align}
				</p>


				<p align="right">\(\Box\)</p>

				<p>
					Write \(\bfm = \bfm_N, \bfS = \bfS_N, \bfA = \phi(\bfx)^T, \bfb = 0, \bfL = \frac{1}{\beta}.\)
				</p>
				<p align="right">\(\Box\)</p>
			</div>

			<h3>Unknown Variance</h3>

			<p>
				If \(\beta\) is treated as unkown the conjugate prior is Guassian-gamma distributed and the predictive distribution is a Student's t - distribution.
			</p>

			<!-- <button class="collapsible">Proof.</button>
			<div class="collapsiblecontent">
				<p align="right">\(\Box\)</p>
			</div> -->

			<h2>Curve Fitting</h2>
			<p>
				Consider the model
				\[y = \sin(2 \pi x) + \epsilon\]
				where \(\epsilon \sim \N(0, 1/5)\).
			</p>
			<p>
				We choose
				\[\phi_j(x) = \exp\left(\frac{(x-a_j)^2}{a_j^2}\right), \quad a_j \in \{0, \pm 1, \pm 2, \pm3\}.\]
			</p>
			<p>
				We have observed follwing data
				<div id="images">
					<img src="./res/linearregression/obsdata.svg" width="75%">
				</div>
			</p>
			<p>
				The maximum likelihood estimation looks like
				<div id="images">
					<img src="./res/linearregression/ml.svg" width="75%">
				</div>
			</p>
			<p>
				The Baysian estimation with \(\alpha = 10^{-6}\) like
				<div id="images">
					<img src="./res/linearregression/bayes5.svg" width="48%">
					<img src="./res/linearregression/bayes10.svg" width="48%">

					<img src="./res/linearregression/bayes.svg" width="75%">
				</div>
			</p>
				
			<h2>Bias - Variance Trade-Off</h2>
			<p>
				Define the squared loss as \(L(\bfx, y) =(y(\bfx) - y)^2\) where \(y\) is the observed value at \(\bfx.\)
			</p>
			<p>
				The expected loss is then
				\[\Exp{L} = \int \int L(\bfx, y) p(\bfx, y) \d \bfx \d y.\]
			</p>
			<p>
				We can write
				\begin{align}
				L(\bfx, y) &= (y(\bfx) - \Exp{y|\bfx} + \Exp{y|\bfx} - y)^2 \\
				&= (y(\bfx) - \Exp{y|\bfx})^2  + (\Exp{y|\bfx} - y)^2 \\ &\quad+ 2(y(\bfx)-\Exp{y|\bfx})(\Exp{y|\bfx} - y)
				\end{align}
				and thus
				\[\Exp{L} = \int (y(\bfx) - \Exp{y|\bfx} )^2 p(\bfx) \d \bfx + \int \int (\Exp{y|\bfx}  - y)^2 p(\bfx, y) \d\bfx \d y\]
				Therefore, the loss is smallest if \(y(\bfx) = \Exp{y|\bfx}.\)
			</p>
			<p>
				Let's make our linear regression dependence  on the data \(\D\) explicit
				\[y(\bfx; \D) = y(\bfx; \bfw) = y(\bfx).\]
				
				We can write
				\begin{align}
				(y(\bfx) - \Exp{y|\bfx})^2 &= (y(\bfx) - \Exp[\D]{y(\bfx; \D)} + \Exp[\D]{y(\bfx; \D)} - \Exp{y|\bfx})^2 \\
				&= (y(\bfx; \D) - \Exp[\D]{y(\bfx; \D)})^2 + (\Exp[\D]{y(\bfx; \D)} - \Exp{y|\bfx})^2 \\
				&\quad + 2(y(\bfx; \D) - \Exp[\D]{y(\bfx; \D)})(\Exp[\D]{y(\bfx; \D)} - \Exp{y|\bfx})
				\end{align}

				and obtain
				\begin{align}
				&\Exp[\D]{(y(\bfx;\D) - \Exp{y|\bfx})^2} = \\
				&\quad (\Exp[\D]{y(\bfx; \D)} - \Exp{y|\bfx})^2 + \Exp[\D]{(y(\bfx; \D) - \Exp[\D]{y(\bfx; \D)})^2}
				\end{align}
			</p>
			<p>
				We conclude
				\[\text{expected loss} = \text{bias}^2 + \text{variance} + \text{noise}\]
				where
				\begin{align}
				\text{bias}^2 &= \int (\Exp[\D]{y(\bfx; \D)} - \Exp{y|\bfx})^2 p(\bfx) \d \bfx, \\
				\text{variance} &= \int \Exp[\D]{(y(\bfx; \D) - \Exp[\D]{y(\bfx; \D)})^2} p(\bfx) \d \bfx, \\
				\text{noise} &= 
				\int \int (y - \Exp{y|\bfx})^2 p(\bfx, y) \d \bfx \d y.
				\end{align}
			</p>
		</main>
		<p>
			<a href="./blog.html">Back</a>
		</p>
		</div>
		<div  style="height: 400px; width:100%; display: block; background-color:white;"></div>
	</body>
	
	<script src="./collapsible.js"></script>
</html>