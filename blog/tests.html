<!DOCTYPE html>

<html lang="en">
	<head>
		<meta charset="UTF-8">
		<link rel="stylesheet" type="text/css" href="./blog.css">
		<link rel="icon" type="image/png" href="./res/m.png">
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

		\(
			\newcommand{\Null}{\mathcal{H}_0}
			\newcommand{\N}{\mathcal{N}}
		\)

		<title>Test test test...</title>
	</head>
	<body>
		<div>
		<p>
			<a href="./blog.html">Back</a>
		</p>
		<h1>Test test test...</h1>
		<main>
			<h2>Z - tests</h2>
			<h3>Test for normal mean - known variance</h3>
			<p>
				For \(X = X_1, \dots, X_n \sim \N(\mu,\sigma^2)\), iid.
				\[\Null: \mu = \mu_0\]
				\[\frac{\overline X - \mu_0}{\sigma / \sqrt{n}} \sim \N(0,1)\]
			</p>
			<h3>Binomial test</h3>
			<p>
				If \(X = X_1, \dots, X_n \sim \text{Bernoulli}(p)\) then
				\[\overline X \sim \frac{1}{n}\text{Binom}(n,p) \approx \N(p, p(1-p)/n)\]
				and
				\[\Null: p = p_0\]

				\[\frac{\overline X - p_0}{\sqrt{p_0(1-p_0) / n}} = \frac{k - n p_0}{\sqrt{np_0(1-p_0)}} \approx \N(0,1)\]

				where \(k = \sum_{i=1}^n X_i \sim \text{Binom}(n,p).\)
			</p>


			<h2>Student's t tests</h2>
			<h3>One sample</h3>
			<p>
				For \(X = X_1, \dots, X_n \sim \N(\mu,\sigma^2)\), iid.
				\[\Null: \mu = \mu_0\]
				\[\frac{\overline{X} - \mu_0}{S_X / \sqrt{n}} \sim t_{n-1}\]
			</p>
			<h3>Two independent samples (unpaired)</h3>
			<p><i>Common variance.</i>
				For \(X = X_1, \dots, X_n \sim \N(\mu_X,\sigma^2)\), iid., \(Y = Y_1, \dots, Y_m \sim \N(\mu_Y,\sigma^2)\), iid.
				\[\Null: \mu_X - \mu_Y = w_0\]
				\[
					\frac{\overline X - \overline Y - w_0}{S_p \sqrt{\frac{1}{n} + \frac{1}{m}}} \sim t_{n+m-2}
				\]
				where
				\[S_p = \sqrt{\frac{(n-1)S_X^2 + (m-2)S_Y^2}{n+m-2}}\]
				is the pooled standard deviation. \(S_p^2\) is an unbiased estimated for \(\sigma^2\).
			</p>
			<p>
				Can be applied if \(0.5 < S_X / S_Y < 2\).
			</p>
			<p>
				If \(m=n\)
				\[
					\frac{\overline X - \overline Y - w_0}{S_p \sqrt{\frac{2}{n}}} \sim t_{2n-2},
				\]
				\[S_p = \sqrt{\frac{S_X^2 + S_Y^2}{2}}.\]
			</p>
			<p><i>Unequal variance - Welch test.</i>
				For \(X = X_1, \dots, X_n \sim \N(\mu_X,\sigma_X^2)\), iid., \(Y = Y_1, \dots, Y_m \sim \N(\mu_Y,\sigma_Y^2)\), iid.
				\[\Null: \mu_X - \mu_Y = w_0\]

				\[
				\frac{\overline X - \overline Y - w_0}{\overline S} \approx t_\nu, \quad \text{where} \quad \overline S = \sqrt{\frac{S_X^2}{n} + \frac{S_Y^2}{m}},
				\]
				\[
				\nu = \left(\frac{S_X^2}{n} + \frac{S_Y^2}{m}\right)^2 {\huge/} \left(\frac{(S_X^2/n)^2}{n-1} + \frac{(S_Y^2/m)^2}{m-1} \right)
				\]

				Recommended: \(m \approx n\), \(m,n > 30\).
			</p>

			<h3>Two dependent samples (paired)</h3>
			<p>
				For \(X = X_1, \dots, X_n\), iid., \(Y = Y_1, \dots, Y_n \), iid. Requirement: Differences \(D_i = X_i - Y_i\) are independent and (approximately) \(\overline D \sim \N(\mu_D,S_D^2/n).\)
				\[\Null: \mu_X - \mu_Y = w_0\]

				\[\frac{\overline{D} - w_0}{S_D / \sqrt{n}} \sim t_{n-1}.\]
			</p>


			<h2>F-Tests</h2>
			<h3>Equality of Variance</h3>
			<p>
				For \(X = X_1, \dots, X_n \sim \N(\mu,\sigma_X^2)\), iid., \(Y = Y_1, \dots, Y_m \sim \N(\mu,\sigma_Y^2)\), iid.
				\[\Null: \sigma_X = \sigma_Y\]
				\[\frac{S_X^2}{S_Y^2} \sim F_{n-1, m-1}\]
			</p>

			<h3>one-way ANOVA</h3>
			<p>
				Investigation of influence of <i>one</i> factor on random variable by considering the means. It is the generalisation of the two sample t test.

				\[\Null: \mu_1 = \mu_2 = \dots = \mu_k\]
			</p>
			<p>
				Requirement: Factor levels have to be independent. Resiuduals must be zero-mean normal with same variance (also between groups).
			</p>
			<p>
				It is assumed that the observations belonging to a group are the same (-noise) and that the variance is explained by the factor alone.


				\[y_{ij} = \mu_i + \epsilon_{ij} = \mu + \tau_i + \epsilon_{ij}, \quad i=1,\dots,k, \enspace j=1,\dots,n_i\]
				where
				\(\epsilon_{ij} \sim \N(0,\sigma^2)\) iid., \(\mu_i = \mu + \tau_i\) is the influence of the factor and \(n_1+\cdots+n_k=N.\)
			</p>
			<p>
				We want to find point estimates of \(\mu,  \mu_i\) and \(\tau_i\).

				Let
				\[\overline y_i = \frac{1}{n_i}\sum_{j=1}^{n_i} y_{ij}, \quad s_i^2 = \frac{1}{n_i-1}\sum_{j=1}^{n_i}(y_{ij}-\overline y_i)^2\]

				Then the sample man is
				\[\overline y = \frac{1}{N}\sum_{i=1}^k n_i \overline y_i.\]

				We take \(\hat \mu = \overline y\), \(\hat \mu_i = \overline y_i\) and \(\hat \tau_i = \hat \mu_i - \hat \mu.\)
				The residuals are \(\hat \epsilon_{ij} = y_{ij} - \overline y_i.\)
			</p>
			<p>
				The decomposition of variance is given by
				\begin{align}
				\underbrace{\sum_{n=1}^N (y_{n} - \overline y)^2}_{\text{TSS}} =
				\underbrace{\sum_{n=1}^N (\hat  y_{n} - \overline y)^2}_\text{RegSS} +
				\underbrace{\sum_{n=1}^N (y_n - \overline y)^2}_\text{RSS}
				\end{align}
				In our case \(\hat y_{ij} = \overline y_i\) and so
				\begin{align}
				\text{TSS} &= \sum_{i=1}^k\sum_{j=1}^{n_i} (y_{ij} - \overline y)^2 \\
				\text{RegSS} &= \sum_{i=1}^k n_i (\overline y_i - \overline y)^2 = \sum_{i=1}^k n_i \hat \tau_i^2 \\
				\text{RSS} &= \sum_{i=1}^k \sum_{j=1}^{n_i} \hat \epsilon_{ij}^2 = \sum_{i=1}^k\sum_{j=1}^{n_i} (y_{ij}-\overline y_i)^2 = \sum_{i=1}^k (n_i-1)s_i^2
				\end{align}
			</p>
			<p>
				Under the null we have
				\[
				\text{RegSS} / \sigma^2 \sim \chi^2_{k-1}, \quad \text{RSS} / \sigma^2 \sim \chi^2_{N-k}.\]
				The test statistic is 
				
				\begin{align}
				\frac{\text{between-group var}}{\text{within-group var}} = \frac{\text{explained var}}{\text{unexplained var}} = \frac{\text{RegSS} / (k-1)}{\text{RSS} / (N-k)}
				\end{align}
				which is F-distributed \(\sim F_{k-1,N-k}.\)
			</p>

			<h3>two-way ANOVA / MANOVA</h3>
			<p>
				The test can be extended to multiple factors but gets more complicated.
			</p>

			<p>
				In general the groups should be balanced (same number of observations) and the assumption about normality should not be violated. F-tests are sensitive.
			</p>
			<p>
				With the ANOVA it is tested whether the variance between groups is greater than withing groups. So we can decide if a classifaction is meaningful. 
			</p>
			<p><i>Example.</i> 
				Suppose we wanted to predict the weight of a dog based on a certain set of characteristics of each dog. One way to do that is to explain the distribution of weights by dividing the dog population into groups based on those characteristics. A successful grouping will split dogs such that (a) each group has a low variance of dog weights (meaning the group is relatively homogeneous) and (b) the mean of each group is distinct (if two groups have the same mean, then it isn't reasonable to conclude that the groups are, in fact, separate in any meaningful way).
				<div id="images">
					<img src="res/tests/ANOVA_no_fit.png"  width="30%">
					<img src="res/tests/ANOVA_fair_fit.jpg"  width="30%">
					<img src="res/tests/ANOVA_very_good_fit.jpg"  width="30%">
				</div>
			</p>
			<p>
				Above we have Young vs old, and short-haired vs long-haired; Pet vs Working breed and less athletic vs more athletic; Weight by breed.
			</p>


			<h2>Chi-Squared Tests</h2>
			<h3>Goodness of Fit of a Distribution</h3>
			<p>
				It is assumed that we have a \(k\) categories with true frequency probabilities \(p_i\), \(i=1,\dots,k.\) (multinomial distribution). <br>
				The Pearson's cumulative test statistic is given by
				\[\chi^2 = \sum_{i=1}^k \frac{(O_i-E_i)^2}{E_i} = n \sum_{i=1}^k \frac{(f_i - p_i)^2}{p_i} \approx \chi^2_{k-1}\] where
				\(n\) is the number of observation, \(O_i\) is the number of observations, \(E_i = np_i\) the expected number of observations and \(f_i = O_i/n\) the fraction of observations in group \(i\). That is \[\Null: E \sim \text{Multinomial}(n,p).\]
				For proof of the approximation see <a href="https://arxiv.org/pdf/1808.09171.pdf">here</a>.
 			</p>
 			<p>
 				For the special case of \(p = (p_1,1-p_1)\) we have \(E_1 \sim \text{Binom}(n,p_1),\) \(E_2 = n-E_1.\)
 				Then
 				\begin{align}
 				\chi_2 &= \frac{(O_1 - np_1)^2}{n p_1} + \frac{(n-O_1 - n(1-p_1))^2}{n (1-p_1)} \\
 				&= \left( \frac{O_1 - n p_1}{\sqrt{np_1(1-p_1)}}\right)^2 \sim \chi^2_1
 				\end{align}
 				This test is closely related to the Binomial test.
 			</p>
		</main>
		<p>
			<a href="./blog.html">Back</a>
		</p>
		</div>
		<div  style="height: 400px; width:100%; display: block; background-color:white;"></div>
	</body>
	
	<script src="./collapsible.js"></script>
</html>